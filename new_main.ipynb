{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some weird config required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "def config_paths(user, env_name):\n",
    "    paths = ['',\n",
    "             '/home/{0}/{1}/.env/bin'.format(user, env_name),\n",
    "             '/usr/lib/python35.zip',\n",
    "             '/usr/lib/python3.5',\n",
    "             '/usr/lib/python3.5/plat-x86_64-linux-gnu',\n",
    "             '/usr/lib/python3.5/lib-dynload',\n",
    "             '/home/{0}/{1}/.env/lib/python3.5/site-packages'.format(user, env_name),\n",
    "             '/home/{0}/{1}/.env/lib/python3.5/site-packages/IPython/extensions'.format(user, env_name),\n",
    "             '/home/{0}/.ipython']\n",
    "\n",
    "    for path in paths:\n",
    "        sys.path.append(path)\n",
    "        \n",
    "config_paths('omri', 'my_gpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras import Model\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Convolution2D\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DIR = '../'\n",
    "K = 40\n",
    "SAMPLE_NUM = 128 * K   \n",
    "IMG_SIZE = 197"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_total = np.zeros(shape = (SAMPLE_NUM, IMG_SIZE,IMG_SIZE,3))\n",
    "Y_total = np.zeros(shape = (SAMPLE_NUM,))\n",
    "\n",
    "for category_ind in range(1,129):\n",
    "    cagetory_dir = os.path.join(TRAIN_DIR, str(category_ind))\n",
    "    \n",
    "    cur_image_list = os.listdir(cagetory_dir)\n",
    "    for im_ind, im_name in enumerate(cur_image_list[:K]):\n",
    "        im = cv2.imread(os.path.join(cagetory_dir, im_name))\n",
    "        im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n",
    "        im = cv2.resize(im, (IMG_SIZE, IMG_SIZE))\n",
    "        im = im/255.0\n",
    "        \n",
    "        X_total[(category_ind-1)*K + im_ind] = im\n",
    "        Y_total[(category_ind-1)*K + im_ind] = category_ind - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train-validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_indices = np.random.permutation(SAMPLE_NUM)\n",
    "X_total = X_total[random_indices]\n",
    "Y_total = Y_total[random_indices]\n",
    "\n",
    "train_num = int(SAMPLE_NUM * 0.7)\n",
    "X_train = X_total[:train_num]\n",
    "Y_train = Y_total[:train_num]\n",
    "\n",
    "X_val = X_total[train_num:]\n",
    "Y_val = Y_total[train_num:]\n",
    "\n",
    "Y_val = np_utils.to_categorical(Y_val,128)\n",
    "Y_train = np_utils.to_categorical(Y_train,128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My Alexnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Sequential()\n",
    "# model.add(Convolution2D(32, 3, 3,\n",
    "#                     border_mode='valid',\n",
    "#                     input_shape=(IMG_SIZE, IMG_SIZE ,3)))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# model.add(Convolution2D(32, 3, 3))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# model.add(Convolution2D(32, 3, 3))\n",
    "# model.add(Activation('relu'))\n",
    "\n",
    "# model.add(Convolution2D(32, 3, 3))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(256))\n",
    "# model.add(Activation('relu'))\n",
    "# #model.add(Dropout(0.5))\n",
    "\n",
    "# model.add(Dense(256))\n",
    "# model.add(Activation('relu'))\n",
    "\n",
    "# model.add(Dense(128))\n",
    "# model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "# model = VGG16(include_top=False, input_shape=(IMG_SIZE, IMG_SIZE ,3), \n",
    "#               weights='imagenet', input_tensor=None, pooling=None, classes=128)\n",
    "\n",
    "# last = model.output\n",
    "\n",
    "# x = Flatten()(last)\n",
    "# x = Dense(256, activation='relu')(x)\n",
    "# preds = Dense(128, activation='softmax')(x)\n",
    "\n",
    "# model = Model(model.input, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras import regularizers\n",
    "\n",
    "model = VGG16(include_top=False, weights='imagenet', input_tensor=None, input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
    "                 pooling=None, classes=128)\n",
    "\n",
    "# model = Model(model.input, preds)\n",
    "\n",
    "for layer in model.layers:\n",
    "    layer.trainable = False\n",
    "    \n",
    "last = model.output\n",
    "\n",
    "x = Flatten()(last)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(512, activation='sigmoid',kernel_initializer='he_normal',kernel_regularizer=regularizers.l2(0.001))(x)\n",
    "preds = Dense(128, activation='softmax')(x)\n",
    "\n",
    "model = Model(model.input, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         (None, 197, 197, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 197, 197, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 197, 197, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 98, 98, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 98, 98, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 98, 98, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 49, 49, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 49, 49, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 49, 49, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 49, 49, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 24, 24, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 24, 24, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 24, 24, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 24, 24, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 12, 12, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 12, 12, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 12, 12, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 12, 12, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 6, 6, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 18432)             0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 18432)             0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 512)               9437696   \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 128)               65664     \n",
      "=================================================================\n",
      "Total params: 24,218,048\n",
      "Trainable params: 9,503,360\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = x.reshape( (-1,SIZE,SIZE,1))\n",
    "# input_shape = x[0].shape\n",
    "# x_train = x.astype(\"float32\")\n",
    "# y_train = y_cat\n",
    "# from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# image_gen = ImageDataGenerator(\n",
    "#     #featurewise_center=True,\n",
    "#     #featurewise_std_normalization=True,\n",
    "# #     rescale=1./255,\n",
    "#     rotation_range=2,\n",
    "# #     width_shift_range=.15,\n",
    "# #     height_shift_range=.15,\n",
    "#     horizontal_flip=True)\n",
    "\n",
    "# #training the image preprocessing\n",
    "# image_gen.fit(X_train, augment=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3584 samples, validate on 1536 samples\n",
      "Epoch 1/40\n",
      "3584/3584 [==============================] - 47s 13ms/step - loss: 2.6883 - acc: 0.6970 - val_loss: 3.3627 - val_acc: 0.3978\n",
      "Epoch 2/40\n",
      "3584/3584 [==============================] - 46s 13ms/step - loss: 2.6467 - acc: 0.7076 - val_loss: 3.3361 - val_acc: 0.4160\n",
      "Epoch 3/40\n",
      "3584/3584 [==============================] - 47s 13ms/step - loss: 2.6179 - acc: 0.7109 - val_loss: 3.3163 - val_acc: 0.4232\n",
      "Epoch 4/40\n",
      "3584/3584 [==============================] - 47s 13ms/step - loss: 2.5789 - acc: 0.7204 - val_loss: 3.3015 - val_acc: 0.4147\n",
      "Epoch 5/40\n",
      "3584/3584 [==============================] - 46s 13ms/step - loss: 2.5508 - acc: 0.7280 - val_loss: 3.2820 - val_acc: 0.4251\n",
      "Epoch 6/40\n",
      "3584/3584 [==============================] - 46s 13ms/step - loss: 2.5096 - acc: 0.7313 - val_loss: 3.2662 - val_acc: 0.4206\n",
      "Epoch 7/40\n",
      "3584/3584 [==============================] - 47s 13ms/step - loss: 2.4929 - acc: 0.7400 - val_loss: 3.2419 - val_acc: 0.4303\n",
      "Epoch 8/40\n",
      "3584/3584 [==============================] - 47s 13ms/step - loss: 2.4545 - acc: 0.7475 - val_loss: 3.2306 - val_acc: 0.4245\n",
      "Epoch 9/40\n",
      "3584/3584 [==============================] - 46s 13ms/step - loss: 2.4285 - acc: 0.7525 - val_loss: 3.2227 - val_acc: 0.4245\n",
      "Epoch 10/40\n",
      "3584/3584 [==============================] - 46s 13ms/step - loss: 2.4003 - acc: 0.7536 - val_loss: 3.2005 - val_acc: 0.4225\n",
      "Epoch 11/40\n",
      "3584/3584 [==============================] - 46s 13ms/step - loss: 2.3729 - acc: 0.7609 - val_loss: 3.1854 - val_acc: 0.4336\n",
      "Epoch 12/40\n",
      "3584/3584 [==============================] - 47s 13ms/step - loss: 2.3384 - acc: 0.7679 - val_loss: 3.1739 - val_acc: 0.4245\n",
      "Epoch 13/40\n",
      "3584/3584 [==============================] - 47s 13ms/step - loss: 2.3193 - acc: 0.7676 - val_loss: 3.1608 - val_acc: 0.4271\n",
      "Epoch 14/40\n",
      "3584/3584 [==============================] - 47s 13ms/step - loss: 2.2811 - acc: 0.7793 - val_loss: 3.1381 - val_acc: 0.4264\n",
      "Epoch 15/40\n",
      "3584/3584 [==============================] - 46s 13ms/step - loss: 2.2595 - acc: 0.7826 - val_loss: 3.1345 - val_acc: 0.4316\n",
      "Epoch 16/40\n",
      "3584/3584 [==============================] - 47s 13ms/step - loss: 2.2333 - acc: 0.7796 - val_loss: 3.1158 - val_acc: 0.4336\n",
      "Epoch 17/40\n",
      "3584/3584 [==============================] - 47s 13ms/step - loss: 2.2131 - acc: 0.7905 - val_loss: 3.1128 - val_acc: 0.4408\n",
      "Epoch 18/40\n",
      "3584/3584 [==============================] - 46s 13ms/step - loss: 2.1800 - acc: 0.7902 - val_loss: 3.1013 - val_acc: 0.4342\n",
      "Epoch 19/40\n",
      "3584/3584 [==============================] - 46s 13ms/step - loss: 2.1591 - acc: 0.7991 - val_loss: 3.0844 - val_acc: 0.4401\n",
      "Epoch 20/40\n",
      "3584/3584 [==============================] - 46s 13ms/step - loss: 2.1396 - acc: 0.7985 - val_loss: 3.0789 - val_acc: 0.4427\n",
      "Epoch 21/40\n",
      "3584/3584 [==============================] - 47s 13ms/step - loss: 2.1085 - acc: 0.8128 - val_loss: 3.0587 - val_acc: 0.4499\n",
      "Epoch 22/40\n",
      "3584/3584 [==============================] - 47s 13ms/step - loss: 2.0974 - acc: 0.8094 - val_loss: 3.0539 - val_acc: 0.4434\n",
      "Epoch 23/40\n",
      "3584/3584 [==============================] - 46s 13ms/step - loss: 2.0739 - acc: 0.8170 - val_loss: 3.0410 - val_acc: 0.4368\n",
      "Epoch 24/40\n",
      "3584/3584 [==============================] - 46s 13ms/step - loss: 2.0457 - acc: 0.8211 - val_loss: 3.0242 - val_acc: 0.4473\n",
      "Epoch 25/40\n",
      "3584/3584 [==============================] - 47s 13ms/step - loss: 2.0308 - acc: 0.8237 - val_loss: 3.0213 - val_acc: 0.4479\n",
      "Epoch 26/40\n",
      "3584/3584 [==============================] - 47s 13ms/step - loss: 2.0070 - acc: 0.8206 - val_loss: 3.0142 - val_acc: 0.4486\n",
      "Epoch 27/40\n",
      "3584/3584 [==============================] - 47s 13ms/step - loss: 1.9906 - acc: 0.8251 - val_loss: 2.9987 - val_acc: 0.4512\n",
      "Epoch 28/40\n",
      "3584/3584 [==============================] - 46s 13ms/step - loss: 1.9585 - acc: 0.8362 - val_loss: 2.9928 - val_acc: 0.4531\n",
      "Epoch 29/40\n",
      "3584/3584 [==============================] - 47s 13ms/step - loss: 1.9450 - acc: 0.8312 - val_loss: 2.9846 - val_acc: 0.4460\n",
      "Epoch 30/40\n",
      "3584/3584 [==============================] - 47s 13ms/step - loss: 1.9284 - acc: 0.8368 - val_loss: 2.9726 - val_acc: 0.4499\n",
      "Epoch 31/40\n",
      "3584/3584 [==============================] - 47s 13ms/step - loss: 1.9103 - acc: 0.8446 - val_loss: 2.9728 - val_acc: 0.4512\n",
      "Epoch 32/40\n",
      "3584/3584 [==============================] - 47s 13ms/step - loss: 1.8881 - acc: 0.8538 - val_loss: 2.9593 - val_acc: 0.4590\n",
      "Epoch 33/40\n",
      "3584/3584 [==============================] - 46s 13ms/step - loss: 1.8675 - acc: 0.8555 - val_loss: 2.9502 - val_acc: 0.4492\n",
      "Epoch 34/40\n",
      "3584/3584 [==============================] - 47s 13ms/step - loss: 1.8563 - acc: 0.8544 - val_loss: 2.9457 - val_acc: 0.4544\n",
      "Epoch 35/40\n",
      "3584/3584 [==============================] - 47s 13ms/step - loss: 1.8281 - acc: 0.8661 - val_loss: 2.9348 - val_acc: 0.4590\n",
      "Epoch 36/40\n",
      "3584/3584 [==============================] - 47s 13ms/step - loss: 1.8085 - acc: 0.8669 - val_loss: 2.9304 - val_acc: 0.4622\n",
      "Epoch 37/40\n",
      "3584/3584 [==============================] - 46s 13ms/step - loss: 1.7968 - acc: 0.8677 - val_loss: 2.9257 - val_acc: 0.4557\n",
      "Epoch 38/40\n",
      "3584/3584 [==============================] - 47s 13ms/step - loss: 1.7739 - acc: 0.8744 - val_loss: 2.9170 - val_acc: 0.4577\n",
      "Epoch 39/40\n",
      "3584/3584 [==============================] - 47s 13ms/step - loss: 1.7610 - acc: 0.8658 - val_loss: 2.9098 - val_acc: 0.4648\n",
      "Epoch 40/40\n",
      "3584/3584 [==============================] - 47s 13ms/step - loss: 1.7478 - acc: 0.8758 - val_loss: 2.9011 - val_acc: 0.4603\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0f1bc5f588>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import optimizers\n",
    "from sklearn.utils import class_weight\n",
    "# class_weight = class_weight.compute_class_weight('balanced',\n",
    "#                                              np.unique( Y_total[:train_num]),\n",
    "#                                               Y_total[:train_num])\n",
    "\n",
    "sgd = optimizers.SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "# adam = optimizers.Adam(lr=0.01)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "          optimizer=sgd,\n",
    "          metrics=['accuracy'])\n",
    "\n",
    "# model.fit_generator(image_gen.flow(X_train, Y_train, batch_size=32),\n",
    "#           epochs=40,\n",
    "#           verbose=1,\n",
    "#           class_weight=class_weight,\n",
    "#             validation_data=(X_val, Y_val))\n",
    "\n",
    "\n",
    "model.fit(X_train, Y_train, batch_size=32, \n",
    "          epochs=40,verbose=1,\n",
    "          validation_data=(X_val, Y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384/384 [==============================] - 6s 17ms/step\n",
      "Test score: 8.391303896903992\n",
      "Test accuracy: 0.06510416666666667\n"
     ]
    }
   ],
   "source": [
    "score, accuracy = model.evaluate(X_val, Y_val, verbose=1)\n",
    "predictions = model.predict(X_val)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_json = model.to_json()\n",
    "with open(\"my_vgg.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "model.save_weights(\"my_vgg.h5\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model if required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import model_from_json\n",
    "\n",
    "json_file = open('my_vgg.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"my_vgg.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_total, X_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "del Y_total, Y_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.zeros(shape = (12800, IMG_SIZE,IMG_SIZE,3))\n",
    "indices = np.zeros(12800)\n",
    "test_path = '../test'\n",
    "files_list = os.listdir(test_path)\n",
    "for im_ind, im_name in enumerate(files_list):\n",
    "    im = cv2.imread(os.path.join(test_path, im_name))\n",
    "    im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n",
    "    im = cv2.resize(im, (IMG_SIZE, IMG_SIZE))\n",
    "    im = im/255.0\n",
    "    img_ind = int(im_name.split('.')[0])\n",
    "    indices[im_ind] = img_ind\n",
    "    X_test[im_ind] = im"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fill in missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_imgs = set(range(12800)) - set(indices)\n",
    "for i, loc in enumerate(np.where(indices==0)[0]):\n",
    "    indices[loc] = list(missing_imgs)[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = loaded_model.predict(X_test)\n",
    "y_test = np.argmax(predictions,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'my_vgg.csv'\n",
    "ans = pd.DataFrame({'id': [int(x) for x in indices],\n",
    "                    'predicted': [int(x) for x in y_test + np.ones(len(y_test))]})\n",
    "\n",
    "ans.sort_values(by='id').to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
