{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some weird config required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "def config_paths(user, env_name):\n",
    "    paths = ['',\n",
    "             '/home/{0}/{1}/.env/bin'.format(user, env_name),\n",
    "             '/usr/lib/python35.zip',\n",
    "             '/usr/lib/python3.5',\n",
    "             '/usr/lib/python3.5/plat-x86_64-linux-gnu',\n",
    "             '/usr/lib/python3.5/lib-dynload',\n",
    "             '/home/{0}/{1}/.env/lib/python3.5/site-packages'.format(user, env_name),\n",
    "             '/home/{0}/{1}/.env/lib/python3.5/site-packages/IPython/extensions'.format(user, env_name),\n",
    "             '/home/{0}/.ipython']\n",
    "\n",
    "    for path in paths:\n",
    "        sys.path.append(path)\n",
    "        \n",
    "config_paths('omri', 'my_gpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras import Model\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Convolution2D\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TRAIN_DIR = '../'\n",
    "K = 15\n",
    "SAMPLE_NUM = 128 * K   \n",
    "IMG_SIZE = 224"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_total = np.zeros(shape = (SAMPLE_NUM, IMG_SIZE,IMG_SIZE,3))\n",
    "Y_total = np.zeros(shape = (SAMPLE_NUM,))\n",
    "\n",
    "for category_ind in range(1,129):\n",
    "    cagetory_dir = os.path.join(TRAIN_DIR, str(category_ind))\n",
    "    \n",
    "    cur_image_list = os.listdir(cagetory_dir)\n",
    "    for im_ind, im_name in enumerate(cur_image_list[:K]):\n",
    "        im = cv2.imread(os.path.join(cagetory_dir, im_name))\n",
    "        im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n",
    "        im = cv2.resize(im, (IMG_SIZE, IMG_SIZE))\n",
    "        im = im/255.0\n",
    "        \n",
    "        X_total[(category_ind-1)*K + im_ind] = im\n",
    "        Y_total[(category_ind-1)*K + im_ind] = category_ind - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train-validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "random_indices = np.random.permutation(SAMPLE_NUM)\n",
    "X_total = X_total[random_indices]\n",
    "Y_total = Y_total[random_indices]\n",
    "\n",
    "train_num = int(SAMPLE_NUM * 0.7)\n",
    "X_train = X_total[:train_num]\n",
    "Y_train = Y_total[:train_num]\n",
    "\n",
    "X_val = X_total[train_num:]\n",
    "Y_val = Y_total[train_num:]\n",
    "\n",
    "Y_val = np_utils.to_categorical(Y_val,128)\n",
    "Y_train = np_utils.to_categorical(Y_train,128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My Alexnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Convolution2D(32, 3, 3,\n",
    "                    border_mode='valid',\n",
    "                    input_shape=(IMG_SIZE, IMG_SIZE ,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Convolution2D(32, 3, 3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Convolution2D(32, 3, 3))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Convolution2D(32, 3, 3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256))\n",
    "model.add(Activation('relu'))\n",
    "#model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(256))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "model = VGG16(include_top=False, input_shape=(IMG_SIZE, IMG_SIZE ,3), \n",
    "              weights='imagenet', input_tensor=None, pooling=None, classes=128)\n",
    "\n",
    "last = model.output\n",
    "\n",
    "x = Flatten()(last)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "preds = Dense(128, activation='softmax')(x)\n",
    "\n",
    "model = Model(model.input, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.applications.resnet50 import ResNet50\n",
    "model = ResNet50(include_top=False, weights='imagenet', input_tensor=None, input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
    "                 pooling=None, classes=128)\n",
    "\n",
    "last = model.output\n",
    "\n",
    "x = Flatten()(last)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "preds = Dense(128, activation='softmax')(x)\n",
    "\n",
    "model = Model(model.input, preds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/omri/gpu_env/.env/lib/python3.5/site-packages/ipykernel_launcher.py:12: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1344 samples, validate on 576 samples\n",
      "Epoch 1/100\n",
      "1344/1344 [==============================] - 58s 43ms/step - loss: 4.8475 - acc: 0.0223 - val_loss: 14.4796 - val_acc: 0.0087\n",
      "Epoch 2/100\n",
      "1344/1344 [==============================] - 47s 35ms/step - loss: 4.4114 - acc: 0.0699 - val_loss: 8.6684 - val_acc: 0.0122\n",
      "Epoch 3/100\n",
      "1344/1344 [==============================] - 47s 35ms/step - loss: 3.7984 - acc: 0.1347 - val_loss: 5.8834 - val_acc: 0.0174\n",
      "Epoch 4/100\n",
      "1344/1344 [==============================] - 46s 35ms/step - loss: 3.0621 - acc: 0.2440 - val_loss: 8.4664 - val_acc: 0.0365\n",
      "Epoch 5/100\n",
      "1344/1344 [==============================] - 46s 35ms/step - loss: 2.4255 - acc: 0.3638 - val_loss: 6.2436 - val_acc: 0.0122\n",
      "Epoch 6/100\n",
      "1344/1344 [==============================] - 46s 35ms/step - loss: 1.6764 - acc: 0.5387 - val_loss: 4.9236 - val_acc: 0.0903\n",
      "Epoch 7/100\n",
      "1344/1344 [==============================] - 46s 35ms/step - loss: 1.0294 - acc: 0.7106 - val_loss: 4.9846 - val_acc: 0.1076\n",
      "Epoch 8/100\n",
      "1344/1344 [==============================] - 46s 35ms/step - loss: 0.6133 - acc: 0.8207 - val_loss: 5.1168 - val_acc: 0.1181\n",
      "Epoch 9/100\n",
      "1344/1344 [==============================] - 47s 35ms/step - loss: 0.4313 - acc: 0.8810 - val_loss: 4.8823 - val_acc: 0.1684\n",
      "Epoch 10/100\n",
      "1344/1344 [==============================] - 47s 35ms/step - loss: 0.2529 - acc: 0.9353 - val_loss: 6.0627 - val_acc: 0.1372\n",
      "Epoch 11/100\n",
      "1344/1344 [==============================] - 46s 35ms/step - loss: 0.1970 - acc: 0.9442 - val_loss: 5.8205 - val_acc: 0.1302\n",
      "Epoch 12/100\n",
      "1344/1344 [==============================] - 47s 35ms/step - loss: 0.1412 - acc: 0.9606 - val_loss: 6.2733 - val_acc: 0.1319\n",
      "Epoch 13/100\n",
      "1344/1344 [==============================] - 47s 35ms/step - loss: 0.0997 - acc: 0.9769 - val_loss: 5.6869 - val_acc: 0.1580\n",
      "Epoch 14/100\n",
      "1344/1344 [==============================] - 47s 35ms/step - loss: 0.0567 - acc: 0.9874 - val_loss: 5.5554 - val_acc: 0.1875\n",
      "Epoch 15/100\n",
      "1344/1344 [==============================] - 47s 35ms/step - loss: 0.0483 - acc: 0.9851 - val_loss: 6.4474 - val_acc: 0.1649\n",
      "Epoch 16/100\n",
      "1344/1344 [==============================] - 47s 35ms/step - loss: 0.0632 - acc: 0.9851 - val_loss: 5.3538 - val_acc: 0.1649\n",
      "Epoch 17/100\n",
      "1344/1344 [==============================] - 47s 35ms/step - loss: 0.0731 - acc: 0.9769 - val_loss: 5.7469 - val_acc: 0.1684\n",
      "Epoch 18/100\n",
      "1344/1344 [==============================] - 47s 35ms/step - loss: 0.0903 - acc: 0.9777 - val_loss: 6.1055 - val_acc: 0.1510\n",
      "Epoch 19/100\n",
      "1344/1344 [==============================] - 47s 35ms/step - loss: 0.1868 - acc: 0.9472 - val_loss: 7.4049 - val_acc: 0.1024\n",
      "Epoch 20/100\n",
      "1344/1344 [==============================] - 47s 35ms/step - loss: 0.2573 - acc: 0.9249 - val_loss: 6.8712 - val_acc: 0.0938\n",
      "Epoch 21/100\n",
      "1344/1344 [==============================] - 47s 35ms/step - loss: 0.3874 - acc: 0.8765 - val_loss: 7.7497 - val_acc: 0.1007\n",
      "Epoch 22/100\n",
      "1344/1344 [==============================] - 47s 35ms/step - loss: 0.2367 - acc: 0.9338 - val_loss: 7.1995 - val_acc: 0.1163\n",
      "Epoch 23/100\n",
      "1344/1344 [==============================] - 46s 35ms/step - loss: 0.1309 - acc: 0.9554 - val_loss: 6.6612 - val_acc: 0.1146\n",
      "Epoch 24/100\n",
      "1344/1344 [==============================] - 47s 35ms/step - loss: 0.0882 - acc: 0.9769 - val_loss: 8.0974 - val_acc: 0.1042\n",
      "Epoch 25/100\n",
      "1344/1344 [==============================] - 47s 35ms/step - loss: 0.0676 - acc: 0.9844 - val_loss: 6.2348 - val_acc: 0.1771\n",
      "Epoch 26/100\n",
      "1344/1344 [==============================] - 46s 35ms/step - loss: 0.0446 - acc: 0.9866 - val_loss: 6.2641 - val_acc: 0.1927\n",
      "Epoch 27/100\n",
      "1344/1344 [==============================] - 47s 35ms/step - loss: 0.0390 - acc: 0.9903 - val_loss: 7.2240 - val_acc: 0.1146\n",
      "Epoch 28/100\n",
      "1344/1344 [==============================] - 46s 35ms/step - loss: 0.0411 - acc: 0.9903 - val_loss: 5.5925 - val_acc: 0.1736\n",
      "Epoch 29/100\n",
      "1344/1344 [==============================] - 47s 35ms/step - loss: 0.0161 - acc: 0.9970 - val_loss: 5.3956 - val_acc: 0.2049\n",
      "Epoch 30/100\n",
      "1344/1344 [==============================] - 47s 35ms/step - loss: 0.0053 - acc: 0.9993 - val_loss: 5.4575 - val_acc: 0.2188\n",
      "Epoch 31/100\n",
      "1344/1344 [==============================] - 47s 35ms/step - loss: 0.0034 - acc: 0.9985 - val_loss: 5.0171 - val_acc: 0.2326\n",
      "Epoch 32/100\n",
      "1344/1344 [==============================] - 47s 35ms/step - loss: 0.0031 - acc: 0.9993 - val_loss: 4.9723 - val_acc: 0.2396\n",
      "Epoch 33/100\n",
      "1344/1344 [==============================] - 47s 35ms/step - loss: 0.0016 - acc: 1.0000 - val_loss: 4.9892 - val_acc: 0.2431\n",
      "Epoch 34/100\n",
      "1344/1344 [==============================] - 47s 35ms/step - loss: 0.0028 - acc: 0.9985 - val_loss: 5.0343 - val_acc: 0.2396\n",
      "Epoch 35/100\n",
      "1344/1344 [==============================] - 47s 35ms/step - loss: 0.0021 - acc: 0.9993 - val_loss: 5.0735 - val_acc: 0.2396\n",
      "Epoch 36/100\n",
      "1344/1344 [==============================] - 47s 35ms/step - loss: 0.0021 - acc: 0.9993 - val_loss: 5.0767 - val_acc: 0.2431\n",
      "Epoch 37/100\n",
      " 320/1344 [======>.......................] - ETA: 30s - loss: 3.7889e-04 - acc: 1.0000"
     ]
    }
   ],
   "source": [
    "from keras import optimizers\n",
    "\n",
    "sgd = optimizers.SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "adam = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "          optimizer=adam,\n",
    "          metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, Y_train, batch_size=32, \n",
    "          nb_epoch=100,verbose=1,\n",
    "          validation_data=(X_val, Y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "score, accuracy = model.evaluate(X_val, Y_val, verbose=1)\n",
    "predictions = model.predict(X_val)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_test = np.zeros(shape = (12800, 32,32,3))\n",
    "indices = np.zeros(12800)\n",
    "test_path = '../test'\n",
    "files_list = os.listdir(test_path)\n",
    "for im_ind, im_name in enumerate(files_list):\n",
    "    im = cv2.imread(os.path.join(test_path, im_name))\n",
    "    im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n",
    "    im = cv2.resize(im, (32, 32))\n",
    "    im = im/255.0\n",
    "    img_ind = int(im_name.split('.')[0])\n",
    "    indices[im_ind] = img_ind\n",
    "    X_test[im_ind] = im"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fill in missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "missing_imgs = set(range(12800)) - set(indices)\n",
    "for i, loc in enumerate(np.where(indices==0)[0]):\n",
    "    indices[loc] = list(missing_imgs)[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)\n",
    "y_test = np.argmax(predictions,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filename = 'sixty_per_cat.csv'\n",
    "ans = pd.DataFrame({'id': [int(x) for x in indices],\n",
    "                    'predicted': [int(x) for x in y_test + np.ones(len(y_test))]})\n",
    "\n",
    "ans.sort_values(by='id').to_csv(filename, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
