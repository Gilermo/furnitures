{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "def config_paths(user, env_name):\n",
    "    paths = ['',\n",
    "             '/home/{0}/{1}/.env/bin'.format(user, env_name),\n",
    "             '/usr/lib/python35.zip',\n",
    "             '/usr/lib/python3.5',\n",
    "             '/usr/lib/python3.5/plat-x86_64-linux-gnu',\n",
    "             '/usr/lib/python3.5/lib-dynload',\n",
    "             '/home/{0}/{1}/.env/lib/python3.5/site-packages'.format(user, env_name),\n",
    "             '/home/{0}/{1}/.env/lib/python3.5/site-packages/IPython/extensions'.format(user, env_name),\n",
    "             '/home/{0}/.ipython']\n",
    "\n",
    "    for path in paths:\n",
    "        sys.path.append(path)\n",
    "        \n",
    "config_paths('omri', 'my_gpu')\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras import Model\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Convolution2D\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras import optimizers\n",
    "\n",
    "TRAIN_DIR = '../'\n",
    "K = 60\n",
    "SAMPLE_NUM = 128 * K   \n",
    "IMG_SIZE = 197\n",
    "\n",
    "from keras.models import model_from_json\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_model:\n",
    "    json_file = open('model.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "\n",
    "    # load weights into new model\n",
    "    loaded_model.load_weights(\"model.h5\")\n",
    "\n",
    "else:\n",
    "    from keras.applications.vgg16 import VGG16\n",
    "    from keras.applications.resnet50 import ResNet50\n",
    "    from keras import regularizers\n",
    "\n",
    "    model = VGG16(include_top=False, weights='imagenet', input_tensor=None, input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
    "                     pooling=None, classes=128)\n",
    "\n",
    "    # model = Model(model.input, preds)\n",
    "\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    last = model.output\n",
    "\n",
    "    x = Flatten()(last)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(512, activation='sigmoid',kernel_initializer='he_normal',kernel_regularizer=regularizers.l2(0.001))(x)\n",
    "    preds = Dense(128, activation='softmax')(x)\n",
    "\n",
    "    model = Model(model.input, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train on all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_on_chunk(model, chunk_number):\n",
    "    global K\n",
    "    global TRAIN_DIR\n",
    "    global SAMPLE_NUM\n",
    "    global IMG_SIZE\n",
    "    X_total = np.zeros(shape = (SAMPLE_NUM, IMG_SIZE,IMG_SIZE,3))\n",
    "    Y_total = np.zeros(shape = (SAMPLE_NUM,))\n",
    "\n",
    "    for category_ind in range(1,129):\n",
    "        cagetory_dir = os.path.join(TRAIN_DIR, str(category_ind))\n",
    "\n",
    "        cur_image_list = os.listdir(cagetory_dir)\n",
    "        if (chunk_number+1)*K > len(cur_image_list):\n",
    "            temp_list = cur_image_list[chunk_number*K:]\n",
    "        else:\n",
    "            temp_list = cur_image_list[chunk_number*K:(chunk_number+1)*K]\n",
    "        for im_ind, im_name in enumerate(temp_list):\n",
    "            im = cv2.imread(os.path.join(cagetory_dir, im_name))\n",
    "            im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n",
    "            im = cv2.resize(im, (IMG_SIZE, IMG_SIZE))\n",
    "            im = im/255.0\n",
    "\n",
    "            X_total[(category_ind-1)*K + im_ind] = im\n",
    "            Y_total[(category_ind-1)*K + im_ind] = category_ind - 1\n",
    "\n",
    "    random_indices = np.random.permutation(SAMPLE_NUM)\n",
    "    X_total = X_total[random_indices]\n",
    "    Y_total = Y_total[random_indices]\n",
    "\n",
    "    train_num = int(SAMPLE_NUM * 0.7)\n",
    "    X_train = X_total[:train_num]\n",
    "    Y_train = Y_total[:train_num]\n",
    "\n",
    "    X_val = X_total[train_num:]\n",
    "    Y_val = Y_total[train_num:]\n",
    "\n",
    "    Y_val = np_utils.to_categorical(Y_val,128)\n",
    "    Y_train = np_utils.to_categorical(Y_train,128)\n",
    "\n",
    "    \n",
    "    earlystopper = EarlyStopping(patience=5, verbose=1)\n",
    "    checkpointer = ModelCheckpoint('model_iter_dropout.h5', verbose=1, save_best_only=True)\n",
    "    \n",
    "    epochs = 300\n",
    "#     learning_rate = 0.001\n",
    "#     decay_rate = learning_rate / epochs\n",
    "\n",
    "    sgd = optimizers.SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    # adam = optimizers.Adam(lr=0.01)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=sgd,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "    # model.fit_generator(image_gen.flow(X_train, Y_train, batch_size=32),\n",
    "    #           epochs=40,\n",
    "    #           verbose=1,\n",
    "    #           class_weight=class_weight,\n",
    "    #             validation_data=(X_val, Y_val))\n",
    "\n",
    "\n",
    "    model.fit(X_train, Y_train, batch_size=32, \n",
    "              epochs=40,verbose=1,\n",
    "              validation_data=(X_val, Y_val))\n",
    "    \n",
    "    \n",
    "    \n",
    "    model.fit(X_train, Y_train, batch_size=32, \n",
    "              nb_epoch=epochs,verbose=1,\n",
    "              validation_data=(X_val, Y_val), callbacks=[checkpointer])\n",
    "    \n",
    "    \n",
    "    \n",
    "for chunk in range(4):\n",
    "    train_on_chunk(model, chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_json = model.to_json()\n",
    "with open(\"my_vgg_all.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "model.save_weights(\"my_vgg_all.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
